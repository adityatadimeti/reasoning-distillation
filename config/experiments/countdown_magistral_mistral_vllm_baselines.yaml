# Pass@K experiment configuration for Countdown puzzles using Magistral Small 2506 for reasoning and Mistral Small 3.1 for summarization via vLLM
experiment_name: "countdown_magistral_mistral_vllm_baselines"
results_dir: "./results/countdown_magistral_mistral_vllm_baselines"
data_path: "./data/countdown_custom_2.csv"
save_intermediate: true
experiment_type: "pass_k"
pass_k_iterations: 5

# Dashboard configuration
dashboard_port: 8080

# Model configuration for vLLM
# DeepSeek-RL for reasoning (running on port 8001)
reasoning_model: "mistralai/Magistral-Small-2506"
reasoning_model_provider: "vllm"

# vLLM server configuration for reasoning model (DeepSeek on GPU 1)
vllm_config:
  host: "localhost"
  port: 8004
  max_model_len: 32768

# Generation parameters for reasoning
max_tokens: 35000  # Extended reasoning trace
temperature: 0.6
top_p: 0.95
top_k: 40
presence_penalty: 0.0
frequency_penalty: 0.0

# Continuation parameters for long reasoning traces
enable_continuation: false  # Disable to avoid context accumulation bug
max_total_tokens: 35000
max_continuations: 1  # Set to 1 to avoid "maximum continuation limit (0)" error

# Answer extraction - use Countdown-specific extractor
answer_extractor: "countdown"

# Prompt configurations
prompts:
  reasoning: "countdown"  # References config/prompts/reasoning.yaml