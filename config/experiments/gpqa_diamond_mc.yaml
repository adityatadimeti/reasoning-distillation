# Experiment configuration for multiple-choice GPQA dataset
experiment_name: "summarization_8_iter_rzn-R1_summ-V3-approach_focused_gpqa_diamond_mc"
results_dir: "./results/summarization_8_iter_rzn-R1_summ-V3_gpqa_diamond_mc"
data_path: "./data/gpqa_diamond_multichoice.csv"
save_intermediate: true

# Dashboard configuration
dashboard_port: 8080

# Model configuration
reasoning_model: "accounts/fireworks/models/deepseek-r1"
summarizer_type: "external"
summarizer_model: "accounts/fireworks/models/deepseek-v3"

# Generation parameters
max_tokens: 32768
temperature: 0.6
top_p: 0.95
top_k: 40
presence_penalty: 0.0
frequency_penalty: 0.0

# Iteration parameters
max_iterations: 8
continue_after_correct: true

# Summarization parameters
enable_summarization: true
summary_max_tokens: 32768
summary_temperature: 0.6
summary_top_p: 0.95
summary_top_k: 40
summary_presence_penalty: 0.0
summary_frequency_penalty: 0.0

# Custom answer extraction for multiple-choice
use_multiple_choice_extractor: true
answer_extractor_path: "mc_answer_extractor.py"
answer_extractor_function: "extract_mc_answer"

# Prompt configurations
prompts:
  reasoning: "gpqa_mc"  # Updated to use a multiple-choice prompt
  summarize: "approach_focused_summarization"  # This references the version in config/prompts/summarize.yaml
  improved: "gpqa_mc"  # Updated to use a multiple-choice prompt 