# Experiment configuration
experiment_name: "aime_deepseek_qwen_7b_wait"
results_dir: "./results/aime_deepseek_qwen_7b_wait"
data_path: "./data/aime_2024_5.csv"
save_intermediate: true
experiment_type: "continuation" # Set experiment type

# Model configuration
reasoning_model_provider: "fireworks"
# Using the model from aime_deepseek_qwen_1p5b_cont_self_sum.yaml as requested
reasoning_model: "accounts/vivek-vajipey-84a360/deployedModels/deepseek-r1-distill-qwen-7b-bad1b741"

# Generation parameters
# Setting max_tokens relatively low to potentially test hitting the limit before </think>
# Note: ContinuationExperiment controls continuation by finding </think> and adding "Wait",
# it disables the client's built-in continuation logic.
max_tokens: 8192
temperature: 0.6
top_p: 0.95
top_k: 40
presence_penalty: 0.0
frequency_penalty: 0.0

max_sub_iterations: 4

# Iteration parameters for ContinuationExperiment
max_iterations: 5  # Run iterations 0, 1, and 2
continue_after_correct: true # Optional: Set to false to stop after first correct answer

# Prompt configuration (Only reasoning prompt needed)
prompts:
  reasoning: "aime" # References config/prompts/reasoning.yaml

answer_extractor: "math" 