# Experiment configuration for DeepSeek-RL reasoning with Qwen2.5 summarization via vLLM
experiment_name: "aime_deepseek_rl_qwen2_5_summ_vllm"
results_dir: "./results/aime_deepseek_rl_qwen2_5_summ_vllm"
data_path: "./data/aime_2024_5.csv"
save_intermediate: true
experiment_type: "summarize"

# Dashboard configuration
dashboard_port: 8080

# Model configuration for vLLM
# DeepSeek-RL for reasoning (running on port 8001)
reasoning_model: "deepseek-ai/DeepSeek-RL-Distill-Qwen-14B"
reasoning_model_provider: "vllm"

# Qwen2.5 for summarization (running on port 8000)
summarizer_type: "external"
summarizer_model: "Qwen/Qwen2-5-14B-Instruct"
summarizer_model_provider: "vllm"

# vLLM server configuration for reasoning model (DeepSeek on GPU 1)
vllm_config:
  host: "localhost"
  port: 8001
  max_model_len: 32768

# vLLM server configuration for summarizer model (Qwen on GPU 0)
summarizer_vllm_config:
  host: "localhost"
  port: 8000
  max_model_len: 32768

# Generation parameters for reasoning
max_tokens: 8192
temperature: 0.6
top_p: 0.95
top_k: 40
presence_penalty: 0.0
frequency_penalty: 0.0

# Continuation parameters for long reasoning traces
enable_continuation: true
max_total_tokens: 32768
max_continuations: 4

# Iteration parameters
max_iterations: 4
continue_after_correct: true

# Summarization parameters
enable_summarization: true
enable_final_summarization: true
allow_fallback: true
extract_post_think_summary: false
summary_max_tokens: 8192
summary_temperature: 0.6
summary_top_p: 0.95
summary_top_k: 40
summary_presence_penalty: 0.0
summary_frequency_penalty: 0.0

# Summary continuation parameters
summary_max_total_tokens: 32768
summary_max_continuations: 4

# Answer extraction
answer_extractor: "math"

# Prompt configurations
prompts:
  reasoning: "aime"  # References config/prompts/reasoning.yaml
  summarize: "aime_summary"  # References config/prompts/summarize.yaml
  improved: "aime_your_approach"  # References config/prompts/improved.yaml