# Configuration for DeepSeek R1 Distill Llama 8B

# Pipeline configuration
pipeline:
  name: external_summary
  type: summary
  batch_size: 4
  save_intermediates: true
  max_iterations: 2
  continuation_prompt: "Based on the summary of your previous reasoning, continue solving the problem. Try to correct any errors identified in the summary."

# Summarization configuration
summarization:
  method: external
  strategy: error_focused
  prompt: error_focused
  max_tokens: 1000
  temperature: 0.7

reasoning_model:
  name: deepseek-r1-distill-llama-8b
  model_id: accounts/fireworks/models/deepseek-r1-distill-llama-8b
  api_type: fireworks
  api_base: https://api.fireworks.ai/inference/v1
  # API key should be in environment variable FIREWORKS_API_KEY
  
  # Model parameters
  max_tokens: 2000
  temperature: 0.7
  top_p: 1.0
  top_k: 40
  frequency_penalty: 0.0
  presence_penalty: 0.0
  
  # API settings
  timeout: 30  # API timeout in seconds
  retries: 3   # Number of retries for API calls
  retry_delay: 2  # Delay between retries in seconds

# Summarization model configuration (self-summarization by default)
summarization_model:
  name: deepseek-r1-distill-llama-8b
  model_id: accounts/fireworks/models/deepseek-r1-distill-llama-8b
  api_type: fireworks
  api_base: https://api.fireworks.ai/inference/v1
  
  # Model parameters - lower temperature for more focused summaries
  max_tokens: 1000
  temperature: 0.5
  top_p: 1.0
  top_k: 40
  frequency_penalty: 0.0
  presence_penalty: 0.0

# Reasoning settings
reasoning:
  think_tag: true
  max_extensions: 8  # Reasonable number of extensions for this model
  target_token_count: 2000